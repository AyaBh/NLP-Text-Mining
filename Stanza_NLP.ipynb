{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanza – a Python library for processing many languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Stanza](https://stanfordnlp.github.io/stanza/) is a Python library for natural language processing that provides pre-trained language models for [almost 70 languages](https://stanfordnlp.github.io/stanza/available_models.html).\n",
    "\n",
    "Stanza language models can perform tasks such as tokenization, part-of-speech tagging, morphological tagging and dependency parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Stanza library\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process a particular language, we must first download a Stanza language model using the `download()`. To download a language model for a given language, pass the two-letter language code (e.g. `fr`, `en`, `hi`) for the language as a string object to the `lang` argument.\n",
    "\n",
    "For example, the following code would download a model for French.\n",
    "\n",
    "```python\n",
    "# Download Stanza language model for French\n",
    "stanza.download(lang='fr')\n",
    "```\n",
    "\n",
    "Stanza delivers models that have been trained on various datasets for some languages. Stanza refers to models that have been trained on various datasets as *packages*. By default, the package containing the model trained on the biggest dataset available for the language in question is downloaded.\n",
    "\n",
    "To select a model trained on a specific dataset, pass the name of its package as a string object to the `package` argument.\n",
    "\n",
    "The list of supported languages and their corresponding package names are provided in [the list of language models](https://stanfordnlp.github.io/stanza/available_models.html) available for Stanza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install the language model into permanent storage, we must supply the model_dir parameter to the download() function, which contains a string pointing to a permanent storage location.\n",
    "\n",
    "If the models are not saved permanently, they will be deleted when the server shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95fb6bbd734f4455946ef8e3fad5e267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 10:39:35 INFO: Downloading default packages for language: fr (French)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6399611ec4184badb3ab956782007e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-fr/resolve/v1.3.0/models/default.zip:   0%|          | 0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 10:41:47 INFO: Finished downloading models and saved to ../stanza_models.\n"
     ]
    }
   ],
   "source": [
    "# Download a Stanza language model for French into the directory \"../stanza_models\"\n",
    "stanza.download(lang='fr', model_dir='../stanza_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a language model into Stanza\n",
    "\n",
    "To load a Stanza language model into Python, we first create a *Pipeline* object by initialising an instance of the `Pipeline()` class from the `stanza` module.\n",
    "\n",
    "Because we did **not** place the language model into the default directory, we also provide a string containing the path to the directory with Stanza language models to the `dir` argument.\n",
    "\n",
    "We then store the pipeline under the variable `nlp_fr`. To load a language model for French into the pipeline, we must provide the string `fr` to the `lang` argument of the `Pipeline()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 10:41:54 INFO: Loading these models for language: fr (French):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "| ner       | wikiner |\n",
      "=======================\n",
      "\n",
      "2021-12-01 10:41:54 INFO: Use device: cpu\n",
      "2021-12-01 10:41:54 INFO: Loading: tokenize\n",
      "2021-12-01 10:41:54 INFO: Loading: mwt\n",
      "2021-12-01 10:41:54 INFO: Loading: pos\n",
      "2021-12-01 10:41:54 INFO: Loading: lemma\n",
      "2021-12-01 10:41:54 INFO: Loading: depparse\n",
      "2021-12-01 10:41:55 INFO: Loading: ner\n",
      "2021-12-01 10:41:56 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Stanza pipeline with a language model for French, which is assigned to the variable 'nlp_fr' using the Pipeline() class.\n",
    "nlp_fr = stanza.Pipeline(lang='fr', dir='../stanza_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a language model into Stanza returns *Pipeline* object, which consists of a number of *processors* that perform various natural language processing tasks.\n",
    "\n",
    "To speed up processing, we can define the processors to be included in the *Pipeline* object by providing the argument `processors` with a string object that contains the [processor names](https://stanfordnlp.github.io/stanza/pipeline.html#processors) to be included in the pipeline.\n",
    "\n",
    "For example, creating a *Pipeline* using the command below would only include the processors for tokenization and part-of-speech tagging into the pipeline.\n",
    "\n",
    "```python\n",
    "# Initialise a Stanza pipeline with a language model for French;\n",
    "# assign model to variable 'nlp_fr'. \n",
    "# Only include tokenizer and part-of-speech tagger.\n",
    "nlp_fr = stanza.Pipeline(lang='fr', processors='tokenize, pos')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing text using Stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can feed some text in French to the models as a string object.\n",
    "\n",
    "We store the result under the variable `doc_fr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed French text to the model under 'nlp_fr'; store result under the variable 'doc_fr'\n",
    "doc_fr = nlp_fr(\"J'aime les chiens. J'aime aussi les chiens. Ce sont des animaux intelligents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a Stanza [*Document*](https://stanfordnlp.github.io/stanza/data_objects.html#document) object, which contains the linguistic annotations created by passing the text through the pipeline.\n",
    "\n",
    "The attribute `sentences` of a Stanza *Document* object contains a list, where each item contains a single sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"J'\",\n",
      "      \"lemma\": \"il\",\n",
      "      \"upos\": \"PRON\",\n",
      "      \"feats\": \"Number=Sing|Person=1|PronType=Prs\",\n",
      "      \"head\": 2,\n",
      "      \"deprel\": \"nsubj\",\n",
      "      \"start_char\": 0,\n",
      "      \"end_char\": 2,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"text\": \"aime\",\n",
      "      \"lemma\": \"aimer\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"feats\": \"Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"start_char\": 2,\n",
      "      \"end_char\": 6,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"text\": \"les\",\n",
      "      \"lemma\": \"le\",\n",
      "      \"upos\": \"DET\",\n",
      "      \"feats\": \"Definite=Def|Number=Plur|PronType=Art\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"det\",\n",
      "      \"start_char\": 7,\n",
      "      \"end_char\": 10,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"text\": \"chiens\",\n",
      "      \"lemma\": \"chien\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"feats\": \"Gender=Masc|Number=Plur\",\n",
      "      \"head\": 2,\n",
      "      \"deprel\": \"obj\",\n",
      "      \"start_char\": 11,\n",
      "      \"end_char\": 17,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"text\": \".\",\n",
      "      \"lemma\": \".\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"head\": 2,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 17,\n",
      "      \"end_char\": 18,\n",
      "      \"ner\": \"O\"\n",
      "    }\n",
      "  ],\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"J'\",\n",
      "      \"lemma\": \"il\",\n",
      "      \"upos\": \"PRON\",\n",
      "      \"feats\": \"Number=Sing|Person=1|PronType=Prs\",\n",
      "      \"head\": 2,\n",
      "      \"deprel\": \"nsubj\",\n",
      "      \"start_char\": 19,\n",
      "      \"end_char\": 21,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"text\": \"aime\",\n",
      "      \"lemma\": \"aimer\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"feats\": \"Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"start_char\": 21,\n",
      "      \"end_char\": 25,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"text\": \"aussi\",\n",
      "      \"lemma\": \"aussi\",\n",
      "      \"upos\": \"ADV\",\n",
      "      \"head\": 2,\n",
      "      \"deprel\": \"advmod\",\n",
      "      \"start_char\": 26,\n",
      "      \"end_char\": 31,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"text\": \"les\",\n",
      "      \"lemma\": \"le\",\n",
      "      \"upos\": \"DET\",\n",
      "      \"feats\": \"Definite=Def|Number=Plur|PronType=Art\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"det\",\n",
      "      \"start_char\": 32,\n",
      "      \"end_char\": 35,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"text\": \"chiens\",\n",
      "      \"lemma\": \"chien\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"feats\": \"Gender=Masc|Number=Plur\",\n",
      "      \"head\": 2,\n",
      "      \"deprel\": \"obj\",\n",
      "      \"start_char\": 36,\n",
      "      \"end_char\": 42,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 6,\n",
      "      \"text\": \".\",\n",
      "      \"lemma\": \".\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"head\": 2,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 42,\n",
      "      \"end_char\": 43,\n",
      "      \"ner\": \"O\"\n",
      "    }\n",
      "  ],\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"Ce\",\n",
      "      \"lemma\": \"ce\",\n",
      "      \"upos\": \"PRON\",\n",
      "      \"feats\": \"Gender=Masc|Person=3|PronType=Dem\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"nsubj\",\n",
      "      \"start_char\": 44,\n",
      "      \"end_char\": 46,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"text\": \"sont\",\n",
      "      \"lemma\": \"être\",\n",
      "      \"upos\": \"AUX\",\n",
      "      \"feats\": \"Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"cop\",\n",
      "      \"start_char\": 47,\n",
      "      \"end_char\": 51,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"text\": \"des\",\n",
      "      \"lemma\": \"un\",\n",
      "      \"upos\": \"DET\",\n",
      "      \"feats\": \"Definite=Ind|Number=Plur|PronType=Art\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"det\",\n",
      "      \"start_char\": 52,\n",
      "      \"end_char\": 55,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"text\": \"animaux\",\n",
      "      \"lemma\": \"animal\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"feats\": \"Gender=Masc|Number=Plur\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"start_char\": 56,\n",
      "      \"end_char\": 63,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"text\": \"intelligents\",\n",
      "      \"lemma\": \"intelligent\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"feats\": \"Gender=Masc|Number=Plur\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"amod\",\n",
      "      \"start_char\": 64,\n",
      "      \"end_char\": 76,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 6,\n",
      "      \"text\": \".\",\n",
      "      \"lemma\": \".\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 76,\n",
      "      \"end_char\": 77,\n",
      "      \"ner\": \"O\"\n",
      "    }\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Check the contents of `doc_fr`\n",
    "print(doc_fr, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 10:42:14 WARNING: Language fr package default expects mwt, which has been added\n",
      "2021-12-01 10:42:14 INFO: Loading these models for language: fr (French):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "=======================\n",
      "\n",
      "2021-12-01 10:42:14 INFO: Use device: cpu\n",
      "2021-12-01 10:42:14 INFO: Loading: tokenize\n",
      "2021-12-01 10:42:14 INFO: Loading: mwt\n",
      "2021-12-01 10:42:14 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Sentence 1 tokens ==========\n",
      "id: (1,)\ttext: J'\n",
      "id: (2,)\ttext: aime\n",
      "id: (3,)\ttext: les\n",
      "id: (4,)\ttext: chiens\n",
      "id: (5,)\ttext: .\n",
      "==========Sentence 2 tokens ==========\n",
      "id: (1,)\ttext: J'\n",
      "id: (2,)\ttext: aime\n",
      "id: (3,)\ttext: aussi\n",
      "id: (4,)\ttext: les\n",
      "id: (5,)\ttext: chiens\n",
      "id: (6,)\ttext: .\n",
      "==========Sentence 3 tokens ==========\n",
      "id: (1,)\ttext: Ce\n",
      "id: (2,)\ttext: sont\n",
      "id: (3,)\ttext: des\n",
      "id: (4,)\ttext: animaux\n",
      "id: (5,)\ttext: intelligents\n",
      "id: (6,)\ttext: .\n"
     ]
    }
   ],
   "source": [
    "nlp_fr = stanza.Pipeline(lang='fr', dir='../stanza_models', processors='tokenize')\n",
    "\n",
    "doc_fr = nlp_fr(\"J'aime les chiens. J'aime aussi les chiens. Ce sont des animaux intelligents.\")\n",
    "\n",
    "for i, sentence in enumerate(doc_fr.sentences):\n",
    "    print(f'==========Sentence {i+1} tokens ==========')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"J'aime les chiens.\", \"J'aime aussi les chiens.\", 'Ce sont des animaux intelligents.']\n"
     ]
    }
   ],
   "source": [
    "# To access segmented sentences\n",
    "print([sentence.text for sentence in doc_fr.sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization without Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 10:42:17 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2021-12-01 10:42:17 INFO: Use device: cpu\n",
      "2021-12-01 10:42:17 INFO: Loading: tokenize\n",
      "2021-12-01 10:42:17 ERROR: Cannot load model from ../stanza_models\\en\\tokenize\\combined.pt\n"
     ]
    },
    {
     "ename": "LanguageNotDownloadedError",
     "evalue": "Could not find the model file ../stanza_models\\en\\tokenize\\combined.pt.  The expected model directory ../stanza_models\\en is missing.  Perhaps you need to run stanza.download(\"en\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\stanza\\pipeline\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m                                                                                           \u001b[0mpipeline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m                                                                                           use_gpu=self.use_gpu)\n\u001b[0m\u001b[0;32m    144\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mProcessorRequirementsException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\stanza\\pipeline\\processor.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config, pipeline, use_gpu)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_variant'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_up_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\stanza\\pipeline\\tokenize_processor.py\u001b[0m in \u001b[0;36m_set_up_model\u001b[1;34m(self, config, use_gpu)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\stanza\\models\\tokenization\\trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, vocab, lexicon, dictionary, model_file, use_cuda)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m# load everything from file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\stanza\\models\\tokenization\\trainer.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m             \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../stanza_models\\\\en\\\\tokenize\\\\combined.pt'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mLanguageNotDownloadedError\u001b[0m                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14240/2728573417.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"../stanza_models\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tokenize'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenize_no_ssplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"J'aime les chiens.\\n\\nJ'aime aussi les chiens. Ce sont des animaux intelligents.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'====== Sentence {i+1} tokens ======='\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf'id: {token.id}\\ttext: {token.text}'\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\stanza\\pipeline\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m                         \u001b[1;31m# model files for this language can't be found in the expected directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mLanguageNotDownloadedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mprocessor_name\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresources\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m                         \u001b[1;31m# user asked for a model which doesn't exist for this language?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLanguageNotDownloadedError\u001b[0m: Could not find the model file ../stanza_models\\en\\tokenize\\combined.pt.  The expected model directory ../stanza_models\\en is missing.  Perhaps you need to run stanza.download(\"en\")"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', dir=\"../stanza_models\", processors='tokenize', tokenize_no_ssplit=True)\n",
    "doc = nlp(\"J'aime les chiens.\\n\\nJ'aime aussi les chiens. Ce sont des animaux intelligents.\")\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 10:42:20 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2021-12-01 10:42:20 INFO: Use device: cpu\n",
      "2021-12-01 10:42:20 INFO: Loading: tokenize\n",
      "2021-12-01 10:42:20 ERROR: Cannot load model from ../stanza_models\\en\\tokenize\\combined.pt\n"
     ]
    },
    {
     "ename": "LanguageNotDownloadedError",
     "evalue": "Could not find the model file ../stanza_models\\en\\tokenize\\combined.pt.  The expected model directory ../stanza_models\\en is missing.  Perhaps you need to run stanza.download(\"en\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\stanza\\pipeline\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m                                                                                           \u001b[0mpipeline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m                                                                                           use_gpu=self.use_gpu)\n\u001b[0m\u001b[0;32m    144\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mProcessorRequirementsException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\stanza\\pipeline\\processor.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config, pipeline, use_gpu)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_variant'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_up_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\stanza\\pipeline\\tokenize_processor.py\u001b[0m in \u001b[0;36m_set_up_model\u001b[1;34m(self, config, use_gpu)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\stanza\\models\\tokenization\\trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, vocab, lexicon, dictionary, model_file, use_cuda)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m# load everything from file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\stanza\\models\\tokenization\\trainer.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m             \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../stanza_models\\\\en\\\\tokenize\\\\combined.pt'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mLanguageNotDownloadedError\u001b[0m                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14240/2783508228.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"../stanza_models\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tokenize'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenize_no_ssplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"J'aime les chiens.\\n\\nJ'aime aussi les chiens. Ce sont des animaux intelligents.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'====== Sentence {i+1} tokens ======='\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf'id: {token.id}\\ttext: {token.text}'\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\stanza\\pipeline\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m                         \u001b[1;31m# model files for this language can't be found in the expected directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mLanguageNotDownloadedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mprocessor_name\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresources\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m                         \u001b[1;31m# user asked for a model which doesn't exist for this language?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLanguageNotDownloadedError\u001b[0m: Could not find the model file ../stanza_models\\en\\tokenize\\combined.pt.  The expected model directory ../stanza_models\\en is missing.  Perhaps you need to run stanza.download(\"en\")"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', dir=\"../stanza_models\", processors='tokenize', tokenize_no_ssplit=False)\n",
    "doc = nlp(\"J'aime les chiens.\\n\\nJ'aime aussi les chiens. Ce sont des animaux intelligents.\")\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Part-of-Speech and Morphological Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 10:42:24 WARNING: Language fr package default expects mwt, which has been added\n",
      "2021-12-01 10:42:24 INFO: Loading these models for language: fr (French):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "=======================\n",
      "\n",
      "2021-12-01 10:42:24 INFO: Use device: cpu\n",
      "2021-12-01 10:42:24 INFO: Loading: tokenize\n",
      "2021-12-01 10:42:24 INFO: Loading: mwt\n",
      "2021-12-01 10:42:24 INFO: Loading: pos\n",
      "2021-12-01 10:42:25 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Nous\tupos: PRON\txpos: None\tfeats: Number=Plur|Person=1|PronType=Prs\n",
      "\n",
      "word: avons\tupos: AUX\txpos: None\tfeats: Mood=Ind|Number=Plur|Person=1|Tense=Pres|VerbForm=Fin\n",
      "\n",
      "word: atteint\tupos: VERB\txpos: None\tfeats: Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part\n",
      "\n",
      "word: la\tupos: DET\txpos: None\tfeats: Definite=Def|Gender=Fem|Number=Sing|PronType=Art\n",
      "\n",
      "word: fin\tupos: NOUN\txpos: None\tfeats: Gender=Fem|Number=Sing\n",
      "\n",
      "word: de\tupos: ADP\txpos: None\tfeats: _\n",
      "\n",
      "word: le\tupos: DET\txpos: None\tfeats: Definite=Def|Gender=Masc|Number=Sing|PronType=Art\n",
      "\n",
      "word: sentier\tupos: NOUN\txpos: None\tfeats: Gender=Masc|Number=Sing\n",
      "\n",
      "word: .\tupos: PUNCT\txpos: None\tfeats: _\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eyabe\\miniconda3\\lib\\site-packages\\stanza\\models\\common\\beam.py:86: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  prevK = bestScoresId // numWords\n"
     ]
    }
   ],
   "source": [
    "nlp_fr = stanza.Pipeline(lang='fr', dir='../stanza_models', processors='tokenize, pos')\n",
    "doc_fr = nlp_fr('Nous avons atteint la fin du sentier.')\n",
    "print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in doc_fr.sentences for word in sent.words], sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 10:54:33 WARNING: Language fr package default expects mwt, which has been added\n",
      "2021-12-01 10:54:33 INFO: Loading these models for language: fr (French):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "=======================\n",
      "\n",
      "2021-12-01 10:54:33 INFO: Use device: cpu\n",
      "2021-12-01 10:54:33 INFO: Loading: tokenize\n",
      "2021-12-01 10:54:33 INFO: Loading: mwt\n",
      "2021-12-01 10:54:33 INFO: Loading: lemma\n",
      "2021-12-01 10:54:33 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: J' \tlemma: il\n",
      "word: aime \tlemma: aimer\n",
      "word: les \tlemma: le\n",
      "word: chiens \tlemma: chien\n",
      "word: . \tlemma: .\n",
      "word: Ils \tlemma: il\n",
      "word: sont \tlemma: être\n",
      "word: intelligents \tlemma: intelligent\n",
      "word: . \tlemma: .\n"
     ]
    }
   ],
   "source": [
    "nlp_fr = stanza.Pipeline(lang='fr', dir=\"../stanza_models\", processors='tokenize, lemma')\n",
    "doc_fr = nlp_fr('J\\'aime les chiens. Ils sont intelligents.')\n",
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc_fr.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Syntactic Dependency Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 10:42:29 WARNING: Language fr package default expects mwt, which has been added\n",
      "2021-12-01 10:42:29 INFO: Loading these models for language: fr (French):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "=======================\n",
      "\n",
      "2021-12-01 10:42:29 INFO: Use device: cpu\n",
      "2021-12-01 10:42:29 INFO: Loading: tokenize\n",
      "2021-12-01 10:42:29 INFO: Loading: mwt\n",
      "2021-12-01 10:42:29 INFO: Loading: pos\n",
      "2021-12-01 10:42:29 INFO: Loading: lemma\n",
      "2021-12-01 10:42:29 INFO: Loading: depparse\n",
      "2021-12-01 10:42:30 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1\tword: J'\thead id: 2\thead: aime\tdeprel: nsubj\n",
      "id: 2\tword: aime\thead id: 0\thead: root\tdeprel: root\n",
      "id: 3\tword: les\thead id: 4\thead: chiens\tdeprel: det\n",
      "id: 4\tword: chiens\thead id: 2\thead: aime\tdeprel: obj\n",
      "id: 5\tword: .\thead id: 2\thead: aime\tdeprel: punct\n",
      "id: 1\tword: Ils\thead id: 3\thead: intelligents\tdeprel: nsubj\n",
      "id: 2\tword: sont\thead id: 3\thead: intelligents\tdeprel: cop\n",
      "id: 3\tword: intelligents\thead id: 0\thead: root\tdeprel: root\n",
      "id: 4\tword: .\thead id: 3\thead: intelligents\tdeprel: punct\n"
     ]
    }
   ],
   "source": [
    "nlp_fr = stanza.Pipeline(lang='fr', dir='../stanza_models', processors='tokenize, pos, lemma, depparse')\n",
    "doc_fr = nlp_fr('J\\'aime les chiens. Ils sont intelligents.')\n",
    "print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc_fr.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Named Entities for Sentence and Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 10:42:31 WARNING: Language fr package default expects mwt, which has been added\n",
      "2021-12-01 10:42:31 INFO: Loading these models for language: fr (French):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| ner       | wikiner |\n",
      "=======================\n",
      "\n",
      "2021-12-01 10:42:31 INFO: Use device: cpu\n",
      "2021-12-01 10:42:31 INFO: Loading: tokenize\n",
      "2021-12-01 10:42:31 INFO: Loading: mwt\n",
      "2021-12-01 10:42:31 INFO: Loading: ner\n",
      "2021-12-01 10:42:32 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity: Chris Manning\ttype: PER\n",
      "entity: Université de Stanford\ttype: ORG\n",
      "entity: Bay Area\ttype: LOC\n"
     ]
    }
   ],
   "source": [
    "nlp_fr = stanza.Pipeline(lang='fr', dir='../stanza_models', processors='tokenize, ner')\n",
    "doc_fr = nlp_fr(\"Chris Manning enseigne à l'Université de Stanford. Il vit dans la Bay Area.\")\n",
    "\n",
    "print(*[f'entity: {ent.text}\\ttype: {ent.type}' for sent in doc_fr.sentences for ent in sent.ents], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 10:42:33 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| sentiment | sstplus  |\n",
      "========================\n",
      "\n",
      "2021-12-01 10:42:33 INFO: Use device: cpu\n",
      "2021-12-01 10:42:33 INFO: Loading: tokenize\n",
      "2021-12-01 10:42:33 ERROR: Cannot load model from ../stanza_models\\en\\tokenize\\combined.pt\n"
     ]
    },
    {
     "ename": "LanguageNotDownloadedError",
     "evalue": "Could not find the model file ../stanza_models\\en\\tokenize\\combined.pt.  The expected model directory ../stanza_models\\en is missing.  Perhaps you need to run stanza.download(\"en\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\stanza\\pipeline\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m                                                                                           \u001b[0mpipeline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m                                                                                           use_gpu=self.use_gpu)\n\u001b[0m\u001b[0;32m    144\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mProcessorRequirementsException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\stanza\\pipeline\\processor.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config, pipeline, use_gpu)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_variant'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_up_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\stanza\\pipeline\\tokenize_processor.py\u001b[0m in \u001b[0;36m_set_up_model\u001b[1;34m(self, config, use_gpu)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\stanza\\models\\tokenization\\trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, vocab, lexicon, dictionary, model_file, use_cuda)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m# load everything from file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\stanza\\models\\tokenization\\trainer.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m             \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../stanza_models\\\\en\\\\tokenize\\\\combined.pt'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mLanguageNotDownloadedError\u001b[0m                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14240/2604565414.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnlp_en\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'../stanza_models'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tokenize, sentiment'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdoc_en\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp_en\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'I hate raw tomatoes. I like cats. I really love dogs.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_en\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\stanza\\pipeline\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m                         \u001b[1;31m# model files for this language can't be found in the expected directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mLanguageNotDownloadedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mprocessor_name\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresources\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m                         \u001b[1;31m# user asked for a model which doesn't exist for this language?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLanguageNotDownloadedError\u001b[0m: Could not find the model file ../stanza_models\\en\\tokenize\\combined.pt.  The expected model directory ../stanza_models\\en is missing.  Perhaps you need to run stanza.download(\"en\")"
     ]
    }
   ],
   "source": [
    "nlp_en = stanza.Pipeline(lang='en', dir='../stanza_models', processors='tokenize, sentiment')\n",
    "\n",
    "doc_en = nlp_en('I hate raw tomatoes. I like cats. I really love dogs.')\n",
    "for i, sentence in enumerate(doc_en.sentences):\n",
    "    print(i, sentence.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfacing Stanza with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use some of the Stanza language models in spaCy which can be achieved using a Python library named [spacy-stanza](https://spacy.io/universe/project/spacy-stanza), which interfaces the two libraries.\n",
    "\n",
    "Given that Stanza currently has more pre-trained language models available than spaCy, the spacy-stanza library considerably increases the number of language models available for spaCy.\n",
    "\n",
    "There is, however, **one major limitation**: the language in question must be supported by both [Stanza](https://stanfordnlp.github.io/stanza/available_models.html) and [spaCy](https://spacy.io/usage/models#languages).\n",
    "\n",
    "For example, we cannot use the Stanza language model for Hindi in spaCy, because spaCy does not support the Hindi language.\n",
    "\n",
    "To start using Stanza language models in spaCy, let's start by importing the spacy-stanza library (module name: `spacy_stanza`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the spaCy and spacy-stanza libraries\n",
    "import spacy\n",
    "import spacy_stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This imports both spaCy and spacy-stanza libraries into Python. To continue, we must ensure that we have the Stanza language model for French available as well.\n",
    "\n",
    "As shown above, this model can be downloaded using the following command:\n",
    "\n",
    "```python\n",
    "# Download a Stanza language model for French\n",
    "stanza.download(lang='fr')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with the language model for Hindi above, we download the Stanza language model into the permanent storage on the CSC server.\n",
    "\n",
    "To do so, provide a string object that points towards the directory `../stanza_models` to the argument `model_dir` of the `download()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ff3377dee143388f91159f19a59aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 10:45:12 INFO: Downloading default packages for language: fr (French)...\n",
      "2021-12-01 10:45:14 INFO: File exists: ../stanza_models\\fr\\default.zip.\n",
      "2021-12-01 10:45:27 INFO: Finished downloading models and saved to ../stanza_models.\n"
     ]
    }
   ],
   "source": [
    "# Download a Stanza language model for French into the directory '../stanza_models'\n",
    "stanza.download(lang='fr', model_dir='../stanza_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because spaCy supports [the French language](https://spacy.io/usage/models#languages), we can load Stanza language models for French into spaCy using the spacy-stanza library.\n",
    "\n",
    "This can be achieved using the `load_pipeline()` function available under the `spacy_stanza` module.\n",
    "\n",
    "To load Stanza language model for a given language, you must provide the two-letter code for the language in question (e.g. `fr`) to the argument `name`:\n",
    "\n",
    "```python\n",
    "# Load a Stanza language model for French into spaCy\n",
    "nlp_fr = spacy_stanza.load_pipeline(name='fr')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we did not download the Stanza language models into the default directory, we must also provide the optional argument `dir` to the `load_pipeline()` function.\n",
    "\n",
    "The `dir` argument takes a string object as its input, which must point to the directory that contains Stanza language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 10:45:44 INFO: Loading these models for language: fr (French):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "| ner       | wikiner |\n",
      "=======================\n",
      "\n",
      "2021-12-01 10:45:44 INFO: Use device: cpu\n",
      "2021-12-01 10:45:44 INFO: Loading: tokenize\n",
      "2021-12-01 10:45:45 INFO: Loading: mwt\n",
      "2021-12-01 10:45:45 INFO: Loading: pos\n",
      "2021-12-01 10:45:45 INFO: Loading: lemma\n",
      "2021-12-01 10:45:45 INFO: Loading: depparse\n",
      "2021-12-01 10:45:46 INFO: Loading: ner\n",
      "2021-12-01 10:45:48 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Use the load_pipeline function to load a Stanza model into spaCy.\n",
    "# Assign the result under the variable 'nlp'.\n",
    "nlp_fr = spacy_stanza.load_pipeline(name='fr', dir='../stanza_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we examine the resulting object under the variable `nlp_fr` using Python's `type()` function, we will see that the object is indeed a spaCy *Language* object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, this object behaves just like any other spaCy *Language* object.\n",
    "\n",
    "We feed the text as a string object to the *Language* object under `nlp_fr` and store the result under the variable `doc_fr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the text to the language model under 'nlp_fr', store result under 'doc_fr'\n",
    "doc_fr = nlp_fr(\"J'aime les chiens. J'aime aussi les chiens. Ce sont des animaux intelligents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue by retrieving sentences from the *Doc* object, which are available under the attribute `sents`.\n",
    "\n",
    "The object available under the `sents` attribute is a Python generator that yields *Doc* objects. \n",
    "\n",
    "To examine them, we must catch the objects into a suitable data structure. In this case, the data structure that best fits our needs is a Python list.\n",
    "\n",
    "Hence we cast the output from the generator object under `sents` into a list using the `list()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[J'aime les chiens.,\n",
       " J'aime aussi les chiens.,\n",
       " Ce sont des animaux intelligents.]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get sentences contained in the Doc object 'doc_fr'.\n",
    "# Cast the result into list.\n",
    "sents_fr = list(doc_fr.sents)\n",
    "\n",
    "# Call the variable to check the output\n",
    "sents_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use spaCy's `displacy` submodule to visualise the syntactic dependencies.\n",
    "\n",
    "To do so for the first sentence under `sents_fr`, we must first access the first item in the list using brackets `[0]` as usual.\n",
    "\n",
    "Let's start by checking the type of this object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the type of the first item in the list 'sents_fr'\n",
    "type(sents_fr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the result is a spaCy *Span* object, which is a sequence of *Token* objects contained within a *Doc* object.\n",
    "\n",
    "We can then call the `render` function from the `displacy` submodule to visualise the syntactic dependencies for the *Span* object under `sents_fr[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"fr\" id=\"cef02bdee53a4928ba98f6afb0c7bb5f-0\" class=\"displacy\" width=\"925\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Ce</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">sont</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">des</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">animaux</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">intelligents.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cef02bdee53a4928ba98f6afb0c7bb5f-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,2.0 575.0,2.0 575.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cef02bdee53a4928ba98f6afb0c7bb5f-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cef02bdee53a4928ba98f6afb0c7bb5f-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cef02bdee53a4928ba98f6afb0c7bb5f-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cop</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cef02bdee53a4928ba98f6afb0c7bb5f-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cef02bdee53a4928ba98f6afb0c7bb5f-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cef02bdee53a4928ba98f6afb0c7bb5f-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cef02bdee53a4928ba98f6afb0c7bb5f-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,266.5 L748.0,254.5 732.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the displacy submodule\n",
    "from spacy import displacy\n",
    "\n",
    "# Use the render function to render the first item [0] in the list 'sents_fr'.\n",
    "# Pass the argument 'style' with the value 'dep' to visualise syntactic dependencies.\n",
    "displacy.render(sents_fr[2], style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that spaCy will raise a warning about storing custom attributes when writing the *Doc* object to disk for visualisation.\n",
    "\n",
    "We can also examine the linguistic annotations created for individual *Token* objects within this *Span* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'\n",
      "il\n",
      "nsubj\n",
      "Number=Sing|Person=1|PronType=Prs\n",
      "\n",
      "aime\n",
      "aimer\n",
      "root\n",
      "Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin\n",
      "\n",
      "les\n",
      "le\n",
      "det\n",
      "Definite=Def|Number=Plur|PronType=Art\n",
      "\n",
      "chiens\n",
      "chien\n",
      "obj\n",
      "Gender=Masc|Number=Plur\n",
      "\n",
      ".\n",
      ".\n",
      "punct\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop over each Token object in the Span\n",
    "for token in sents_fr[0]:\n",
    "    \n",
    "    # Print the token, its lemma, dependency and morphological features\n",
    "    print(token, token.lemma_, token.dep_, token.morph, \"\",  sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples above show how we can access the linguistic annotations created by a Stanza language model through spaCy *Doc*, *Span* and *Token* objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a675f174d8b8eca6d029e3cc33ad9ff2549d84420099ad4733e6b330fdaae85e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
